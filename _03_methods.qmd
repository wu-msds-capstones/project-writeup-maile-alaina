# Methods
## Dataset Selection and API Development
We started by selecting a dataset licensed from the Massachusetts Institute of Technology (MIT), where each row indicates the presence or absence of over one hundred symptoms associated with a single diagnosis. Using this dataset, we developed multiple API calls utilizing ChatGPT, a popular generative AI platform, to act as a tele-health doctor. For each row in our dataset, the API presented ChatGPT with a randomized order of symptoms that tested positive and requested its diagnosis estimation. Following the API's execution, we compared ChatGPT's diagnosis with the diagnosis provided in the source dataset. To assess accuracy, we applied a scoring system: one point for an incorrect and unrelated diagnosis, two points for an incorrect but related diagnosis, and three points for a correct diagnosis. This methodology enabled us to evaluate the effectiveness of ChatGPT in accurately predicting diagnoses based on symptom data.

## Data Engineering
### Data Collection
Our data collection process involved several stages and sources:

* Initially, we sourced synthetic symptom and prognosis data from Kaggle, providing a foundational dataset for our project.
* We utilized the ChatGPT API to generate additional data by requesting diagnosis estimations.
* Alongside the diagnosis estimations, we recorded an accuracy score ranging from 1 to 3 to evaluate ChatGPT's diagnostic performance.
### Dataset Description
**Synthetic Source:** The synthetic dataset was sourced from Kaggle's "Disease Prediction Using Machine Learning" competition. It contains two primary CSV files for training and testing, with each file containing 133 columns (132 symptom columns and 1 prognosis column).

####  Content Breakdown:

* **Columns:** 132 columns for symptoms (e.g., itching, skin rash) and 1 column for disease prognosis.
* **Files:** Training.csv for model training and Testing.csv for model evaluation.
### Data Augmentation
To understand the impact of generative AI on prognosis diagnosing, we augmented our data by comparing the accuracy between the initial dataset and generative AI outputs through API calls using OpenAI.

### API Key Setup
We selected OpenAI for this project because its API allows developers to integrate advanced language features into their applications. OpenAI's versatility in chatbots, virtual assistants, and content creation made it ideal for simulating a doctor's role.

### Model Selection
We utilized the "gpt-3.5-turbo" model from OpenAI, known for its efficiency and enhanced capabilities. This model was chosen for its ability to handle complex language tasks with high accuracy.

### Initial Diagnosis Call
Our first API call involved instructing the AI to act as a doctor with the following prompt:


> *"Pretend you are a doctor. Patient presents to you with symptoms: {' '.join(symptoms)}.
> 
> Predict the primary diagnosis concisely using ten words or less."*

### Accuracy Evaluation
We evaluated the predicted prognosis using a custom function get_rating that sends a prognosis and its predicted values to the GPT-4 API and receives a rating based on match accuracy. The rating scale ranges from 1 to 3:

1. No match
2. Same family
3. Perfect match

### Medical Family Determination
We determined the medical "family" for both the original prognosis and the generative AI prognosis using specific prompts to categorize them.

### New Feature Creation
Due to the API calls, the following new features (columns) were created:

* Predicted Values
* Rating
* Medical Family
* Predicted Family

We also created new "calculated columns" to determine a match between the original data and the generative AI data, including match_found and match_found_family.

### Data Cleaning
Data cleaning and preprocessing were essential to ensure the quality and reliability of our dataset. Key steps included:

* Addressing inconsistencies or errors.
* Extracting scales from the "predicted diagnosis" to include only the diagnosis.
* Randomizing the order of symptoms to account for replicates.

### Data Storage
We utilized a GitHub repository for the effective organization and future reference of our data, enabling efficient version control and centralized access.

### Repository Structure and Purpose
* .github: GitHub-specific configurations.
* **Data:** Stores raw and processed datasets.
* **Data_Engineering:** Scripts and resources for data engineering tasks.
* **Documentation:** Project documentation, reports, and written materials.
* **Machine_Learning:** Machine learning models, training scripts, and evaluation metrics.
* **Statistics:** Statisitical modeling, scripts, and evaluation metrics. 
* **.DS_Store:** A system file related to macOS directory management.
* **README.md:** Overview of the project, research question, project description, and links to additional resources.

### Entity-Relationship Diagram (ERD)
Our data is structured in a relational database to maintain a normalized and efficient schema, which is visualized in the following ERD:


## Statistical Methodology
We employed a comprehensive statistical methodology to evaluate the performance of generative AI (ChatGPT) in diagnosing medical conditions. Our approach included descriptive statistics, accuracy metrics, various statistical tests, error analysis, and bias and variability analysis. Hereâ€™s an overview of the statistical methods used:

1. **Descriptive Statistics:**
To summarize the central tendency and dispersion of accuracy scores, we calculated the mean, median, mode, standard deviation, and variance. This provided a foundational understanding of the distribution of diagnostic accuracy ratings.

2. **Confusion Matrix:**
We used a confusion matrix to visualize the AI model's performance by showing the counts of true positives, true negatives, false positives, and false negatives. This helped in understanding the model's strengths and weaknesses in classification tasks.

3. **Accuracy Metrics:**
We calculated precision, recall, and F1 scores to evaluate the model's performance. Precision and recall provided insights into the model's ability to correctly identify positive cases, while the F1 score balanced these metrics to give an overall performance measure.

4. **Chi-Square Test:**
To determine if there was a significant difference between the expected and observed frequencies of diagnostic accuracy categories, we conducted a chi-square test. This helped us assess the model's performance consistency across different diagnostic categories.

5. **Correlation Analysis:**
Correlation analysis was performed to assess the relationships between different features in the dataset and the accuracy of AI predictions. This helped us understand the impact of various features on the model's performance and identify areas for potential improvement.

By employing these statistical methods, we comprehensively evaluated the AI's diagnostic capabilities, providing a robust framework for understanding its accuracy, reliability, and areas for future enhancement. This multi-faceted approach ensured a thorough assessment, contributing valuable insights into the feasibility and effectiveness of using generative AI for healthcare diagnostics.

## Machine Learning
# Model Selection and Development
We developed multiple machine learning models, including logistic regression, decision trees, random forests, support vector machines (SVM), and neural networks to diagnose ailments.

# Addressing Overfitting and Validation
To prevent overfitting and ensure generalizability, we implemented k-fold cross-validation and pruning techniques.

# Model Comparison and Feature Importance
Comparing different models allowed us to identify the most effective algorithm for diagnosing ailments. Analyzing feature importance in the random forest model provided insights into influential symptoms, guiding further refinement of telehealth diagnosis platforms.