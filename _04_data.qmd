# Methods

## Dataset Selection

In this study, we used a dataset obtained from Kaggle, licensed by the Massachusetts Institute of Technology (MIT). The dataset includes over one hundred symptoms linked to various diagnoses, making it well-suited for assessing the diagnostic accuracy of generative AI models. The data was divided into training and testing sets, with each set containing 132 symptom columns and one prognosis column. Specifically, 80% of the data was allocated for training, and the remaining 20% was reserved for testing. This division was intended to ensure a thorough evaluation, allowing the model's generalization capabilities to be accurately measured by testing on previously unseen data.

## API Development

To simulate the role of a telehealth doctor, we employed ChatGPT, a generative AI model, through a series of API calls. Each row in our dataset was processed by presenting ChatGPT with a randomized sequence of symptoms that tested positive. The AI was then prompted to provide a diagnostic prediction based on these symptoms. This process allowed us to collect predictions that were later compared against the actual diagnoses in the dataset.

## Data Engineering

The data engineering process for this study involved several key steps to ensure the quality and reliability of the dataset:

### Data Collection

- **Synthetic Data from Kaggle:** The foundational dataset, containing symptoms and prognoses, was sourced from Kaggle's "Disease Prediction Using Machine Learning" competition.
- **ChatGPT API Calls:** Additional data was generated using the ChatGPT API by requesting diagnostic predictions based on the provided symptoms. This was crucial for evaluating the AI's predictive accuracy. The "gpt-3.5-turbo" model from OpenAI was chosen for its efficiency and advanced capabilities, making it ideal for handling complex language tasks with precision.

![_Figure 7: Flowchart showing steps from dataset selection to diagnosis estimation using API calls._](images/figure_2.png)

### Data Augmentation

- **Generative AI Impact Assessment:** To evaluate the impact of generative AI on diagnostic accuracy, the original dataset’s accuracy was compared with the predictions generated by ChatGPT.
- **Initial Diagnosis Call:** The first API call instructed the AI to act as a doctor with the prompt: "Pretend you are a doctor. Patient presents with symptoms: {symptoms}. Predict the primary diagnosis concisely using ten words or less."
- **Accuracy Evaluation:** The predicted prognosis was evaluated using a custom function `get_rating` that sent the prognosis and predicted values to the GPT-4 API, receiving a rating based on match accuracy. The rating scale ranged from 1 (No match) to 3 (Perfect match).
- **Medical Family Determination:** Specific prompts were used to categorize the original and AI-generated prognoses into their respective medical "families."

![_Figure 8: A visual flow chart illustrating the steps taken to further augment our data._](images/figure_3.png)

### Data Cleaning

- **Inconsistency Resolution:** Inconsistencies in the data were identified and corrected to ensure accurate symptom-disease relationships.
- **Symptom Randomization:** To reduce bias, the order of symptoms presented to ChatGPT was randomized during API calls, ensuring the AI’s predictions were not influenced by symptom order.
- **Data Storage:** We organized the dataset and associated resources in a GitHub repository to ensure efficient version control and centralized access. This repository serves as a comprehensive resource for data management and future reference.

#### Repository Structure:

- `.github`: Contains GitHub-specific configurations.
- `Data`: Houses both raw and processed datasets.
- `Data_Engineering`: Includes scripts and resources for data engineering tasks.
- `Documentation`: Contains project documentation, reports, and written materials.
- `Machine_Learning`: Stores machine learning models, training scripts, and evaluation metrics.
- `Statistics`: Includes statistical modeling scripts and evaluation metrics.
- `README.md`: Provides an overview of the project, including the research question, project description, and links to additional resources.

## Statistical Analysis

To evaluate the performance of ChatGPT’s diagnostic predictions, we employed various statistical methods:

- **Descriptive Statistics:** We calculated metrics such as mean, median, standard deviation, and variance to summarize the accuracy scores and provide an overview of the AI’s performance.
- **Confusion Matrix:** A confusion matrix was used to visualize the AI model's performance by showing the counts of true positives, true negatives, false positives, and false negatives. This provided insights into the accuracy and errors of the AI model.
- **Accuracy Metrics:** Precision, recall, and F1 scores were calculated to evaluate the AI’s performance across different diagnostic categories, offering a more detailed understanding of its accuracy.
- **Chi-Square Test:** To determine whether there was a significant difference between the expected and observed frequencies of diagnostic accuracy categories, we conducted a chi-square test, helping to assess the consistency of the AI’s predictions.
- **Fisher's Exact Test:** In cases where sample sizes were small and the assumptions for the chi-square test might not hold, Fisher’s Exact Test was used to provide a more accurate measure of the significance of the association between the AI’s predictions and actual diagnoses.

## Machine Learning Model Development

To provide a comparative analysis, we developed traditional machine learning models using the same dataset, allowing us to compare their performance against ChatGPT’s:

### Model Development

- **Decision Trees:** Decision Trees were selected for their ability to handle complex datasets with hierarchical relationships. This model helps visualize the decision-making process and understand how specific symptoms influence diagnosis.
- **Logistic Regression:** Logistic Regression was used for its strength in binary classification tasks, providing probabilistic outputs that indicate the likelihood of a diagnosis based on symptoms.

### Model Evaluation

The machine learning models were evaluated using metrics such as accuracy, precision, recall, and F1 scores, providing a basis for comparing their performance with that of the generative AI model.
 